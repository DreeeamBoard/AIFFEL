## 인공지능 작사가

### 1. 데이터 읽어오기


```python
import glob
import os

txt_file_path = os.getenv('HOME')+'/aiffel/Exploration/4. 인공지능 작사가/lyricist/data/lyrics/*'

txt_list = glob.glob(txt_file_path)

raw_corpus = []

# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.
for txt_file in txt_list:
    with open(txt_file, "r") as f:
        raw = f.read().splitlines()
        raw_corpus.extend(raw)

print("데이터 크기:", len(raw_corpus))
print("Examples:\n", raw_corpus[:20])
```

    데이터 크기: 187088
    Examples:
     ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', "Just thinking how you've done me wrong", 'I grew strong', "I learned how to get along And so you're back", 'From outer space', 'I just walked in to find you', 'Here without that look upon your face I should have changed that fucking lock', 'I would have made you leave your key', 'If I had known for just one second', "You'd be back to bother me Well now go,", 'Walk out the door', 'Just turn around', "Now, you're not welcome anymore Weren't you the one", 'Who tried to break me with desire?', "Did you think I'd crumble?", "Did you think I'd lay down and die? Oh not I,", 'I will survive']


### 2. 데이터 정제


```python
# preprocess_sentence()로 데이터 정제
# 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 
```


```python
# 텍스트 분류 모델에서 많이 보신 것처럼 텍스트 생성 모델에도 단어 사전을 만들게 됩니다.
# 그렇다면 문장을 일정한 기준으로 쪼개야겠죠? 그 과정을 토큰화(Tokenize) 라고 합니다.
# 가장 심플한 방법은 띄어쓰기를 기준으로 나누는 방법


def preprocess_sentence(sentence):
    sentence = sentence.lower().strip()
    # 소문자화, 양쪽 공백 삭제
    
    sentence = re.sub(r"([?.!,¿])", r" \1 ", sentence)
    # 특수문제 양쪽에 공백 삽입
    
    sentence = re.sub(r'[" "]+', " ", sentence)
    # 여러개의 공백을 하나의 공백으로
    
    sentence = re.sub(r"[^a-zA-Z?.!,¿]+", " ", sentence)
    # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로
    
    sentence = sentence.strip()
    # 다시 양쪽 공백 삭제
    
    sentence = '<start> ' + sentence + ' <end>'
    # 문장 시작에는 <start>, 끝에는 <end>를 추가
    return sentence

# 이 문장이 어떻게 필터링되는지 확인해 보세요.
print(preprocess_sentence("This @_is ;;;sample        sentence."))
```


```python
# 정제된 문장을 모으자
# 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기
corpus = []

for sentence in raw_corpus:
    # 우리가 원하지 않는 문장은 건너뜁니다
    if len(sentence) == 0: continue
    if sentence[-1] == ":": continue
    
    # 정제를 하고 담아주세요
    preprocessed_sentence = preprocess_sentence(sentence)
    corpus.append(preprocessed_sentence)
        
# 정제된 결과를 10개만 확인해보죠
corpus[:10]
```


```python
# 2-2. Tokenizer 패키지로 corpus를 텐서로 변환
```


```python
# tf.keras.preprocessing.text.Tokenizer 패키지는 정제된 데이터를 토큰화하고,
# 단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며,
# 데이터를 숫자로 변환까지 한 방에 해줍니다.

# 이 과정을 벡터화(vectorize) 라 하며, 숫자로 변환된 데이터를 텐서(tensor)
# 텐서플로우로 만든 모델의 입출력 데이터는 실제로는 모두 이런 텐서로 변환되어 처리되는 것

# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다

def tokenize(corpus):
    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다
    # 우리는 이미 문장을 정제했으니 filters가 필요없어요
    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요
    tokenizer = tf.keras.preprocessing.text.Tokenizer(
        num_words=7000, 
        filters=' ',
        oov_token="<unk>"
    )
    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다
    tokenizer.fit_on_texts(corpus)
    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다
    tensor = tokenizer.texts_to_sequences(corpus) 
    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다
    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.
    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다
    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  
    
    print(tensor,tokenizer)
    return tensor, tokenizer

tensor, tokenizer = tokenize(corpus)
```


```python
# 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력해 봅시다
print(tensor[:3, :10])
tensor.shape
```


```python
# 단어사전 확인
for idx in tokenizer.index_word:
    print(idx, ":", tokenizer.index_word[idx])

    if idx >= 10: break
```


```python
# source, target 문장 생성
src_input = tensor[:, :-1]   # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.
tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.

print(src_input[0])
print(tgt_input[0])
src_input.shape
```

### 3. 평가 데이터셋 분리


```python
from sklearn.model_selection import train_test_split

# tensor를 train, test 데이터로 분리
enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)

print("Source Train:", enc_train.shape)
print("Target Train:", dec_train.shape)
```

### 4. 인공지능 만들기


```python
#Loss
loss = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

dd
```
