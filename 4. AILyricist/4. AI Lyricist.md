## 인공지능 작사가

### 1. 데이터 읽어오기


```python
import glob
import os, re
import numpy as np
import tensorflow as tf

txt_file_path = os.getenv('HOME')+'/aiffel/Exploration/4. 인공지능 작사가/lyricist/data/lyrics/*'
txt_list = glob.glob(txt_file_path)
raw_corpus = []

# glob을 활용하여 txt 파일을 모두 읽어온 후, raw_corpus 리스트에 문장 단위로 저장
for txt_file in txt_list:
    with open(txt_file, "r") as f:
        raw = f.read().splitlines()
        raw_corpus.extend(raw)

print("데이터 크기:", len(raw_corpus))
print("Examples:\n", raw_corpus[:20])
```

    데이터 크기: 187088
    Examples:
     ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', "Just thinking how you've done me wrong", 'I grew strong', "I learned how to get along And so you're back", 'From outer space', 'I just walked in to find you', 'Here without that look upon your face I should have changed that fucking lock', 'I would have made you leave your key', 'If I had known for just one second', "You'd be back to bother me Well now go,", 'Walk out the door', 'Just turn around', "Now, you're not welcome anymore Weren't you the one", 'Who tried to break me with desire?', "Did you think I'd crumble?", "Did you think I'd lay down and die? Oh not I,", 'I will survive']


### 2. 데이터 정제

#### 2-1. Regex로 corpus 생성


```python
# Tokenize : 문장을 일정한 기준으로 쪼개는 것. preprocess_sentence 함수로 해보자

def preprocess_sentence(sentence):
    
    sentence = re.sub(r'\[[^)]*\]', '', sentence)
    # [Chorus]처럼 대괄호로 파트 구분하는 문자 제거
    
    sentence = sentence.lower().strip()
    # 소문자화, 양쪽 공백 삭제
    
    sentence = re.sub(r"([?.!,¿])", r" \1 ", sentence)
    # 특수문제 양쪽에 공백 삽입
    
    sentence = re.sub(r'[" "]+', " ", sentence)
    # 여러개의 공백을 하나의 공백으로
    
    sentence = re.sub(r"[^a-zA-Z?.!,¿]+", " ", sentence)
    # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로
    
    sentence = sentence.strip()
    # 다시 양쪽 공백 삭제
    
    sentence = '<start> ' + sentence + ' <end>'
    # 문장 시작에는 <start>, 끝에는 <end>를 추가
    return sentence

# 예시 문장이 어떻게 필터링되는지 확인
print(preprocess_sentence(raw_corpus[0]))
```

    <start> at first i was afraid <end>



```python
# 정제된 문장을 모으자

corpus = []

for sentence in raw_corpus:
    
    # 원하지 않는 문장은 건너뛰자
    if len(sentence) == 0: continue
        
    if len(sentence.split()) > 15 : continue
    # 너무 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거하자
    # 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 
    
    if preprocess_sentence(sentence) == '<start>  <end>': continue 
    # 공백만 있는 문장 제외
    
    # 정제를 하고 담자
    preprocessed_sentence = preprocess_sentence(sentence)
    corpus.append(preprocessed_sentence)
        
# 정제된 결과 확인
print(len(corpus))
corpus[:10]
```

    168011





    ['<start> at first i was afraid <end>',
     '<start> i was petrified <end>',
     '<start> i kept thinking i could never live without you <end>',
     '<start> by my side but then i spent so many nights <end>',
     '<start> just thinking how you ve done me wrong <end>',
     '<start> i grew strong <end>',
     '<start> i learned how to get along and so you re back <end>',
     '<start> from outer space <end>',
     '<start> i just walked in to find you <end>',
     '<start> here without that look upon your face i should have changed that fucking lock <end>']



#### 2-2. tf.keras.preprocessing.text.Tokenizer 패키지로 corpus를 텐서로 변환


```python
# tf.keras.preprocessing.text.Tokenizer 패키지 :
# 정제된 데이터를 토큰화하고, 단어 사전을 만들어주며, 데이터를 숫자로 변환까지 해준다.

# 이 과정을 벡터화(vectorize) 라 하며, 숫자로 변환된 데이터를 텐서(tensor)

# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용한다.

# num_words 갯수의 단어를 기억할 수 있는 tokenizer
def tokenize(corpus):
    tokenizer = tf.keras.preprocessing.text.Tokenizer(
        num_words=12000, # 전체 단어의 개수
        # 단어장의 크기는 12,000 이상 으로 설정하라는 가이드가 있었다
        filters=' ', # 문장을 이미 정제했으니 filters 필요 없다
        oov_token="<unk>" # 사전에 없었던 단어는 <unk>으로
    )
    
    tokenizer.fit_on_texts(corpus)
    # corpus를 이용해 tokenizer 내부의 단어장을 완성한다
    
    tensor = tokenizer.texts_to_sequences(corpus) 
    # tokenizer를 이용해 corpus를 Tensor로 변환
    
    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', 
                                                           maxlen=15)  

    # padding으로 입력 데이터의 시퀀스 길이를 일정하게 맞추기 (maxlen 15로 설정) 
    # 시퀀스가 짧다면 문장 뒤에 패딩을 붙여서 길이를 맞춰준다
    print(tensor,tokenizer)
    return tensor, tokenizer

tensor, tokenizer = tokenize(corpus)
```

    [[   2   71  250 ...    0    0    0]
     [   2    5   57 ...    0    0    0]
     [   2    5 1104 ...    0    0    0]
     ...
     [   2   47   16 ...    0    0    0]
     [  24    9 2889 ...  261   19    3]
     [   2    6  176 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f81e2b01350>



```python
# 생성된 tensor 확인
print(tensor[:3])
tensor.shape
```

    [[   2   71  250    5   57  644    3    0    0    0    0    0    0    0
         0]
     [   2    5   57 6675    3    0    0    0    0    0    0    0    0    0
         0]
     [   2    5 1104  510    5  104   79  202  257    7    3    0    0    0
         0]]





    (168011, 15)




```python
# 텐서 데이터는 모두 정수로 이루어져 있고
# 이 숫자는 tokenizer에 구축된 단어 사전의 인덱스다

for idx in tokenizer.index_word:
    print(idx, ":", tokenizer.index_word[idx])

    if idx >= 20: break
```

    1 : <unk>
    2 : <start>
    3 : <end>
    4 : ,
    5 : i
    6 : the
    7 : you
    8 : and
    9 : a
    10 : to
    11 : it
    12 : me
    13 : my
    14 : in
    15 : t
    16 : s
    17 : that
    18 : on
    19 : of
    20 : .



```python
# 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게 하자
# source, target 문장 생성

src_input = tensor[:, :-1]  
# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성
# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다

tgt_input = tensor[:, 1:]
# tensor에서 <start>를 잘라내서 타겟 문장을 생성

print(src_input[10])
print(tgt_input[10])
src_input.shape

# 소스는 2(<start>)에서 시작해서 3(<end>)으로 끝난 후 0(<pad>)로 채워져 있다.
# 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한 칸 shift 한 형태를 가지고 있다
```

    [  2   5 135  75 220   7 193  21 745   3   0   0   0   0]
    [  5 135  75 220   7 193  21 745   3   0   0   0   0   0]





    (168011, 14)



### 3. 평가 데이터셋 분리


```python
# tokenize() 함수로 데이터를 Tensor로 변환한 후,
# sklearn 모듈의 train_test_split() 함수로 훈련 데이터와 평가 데이터를 분리

from sklearn.model_selection import train_test_split

# tensor를 train, test 데이터로 분리
enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, 
                                                          tgt_input, test_size=0.2)
# 총 데이터의 20% 를 평가 데이터셋으로 사용하라는 가이드가 있었다

print("Source Train:", enc_train.shape)
print("Target Train:", dec_train.shape)

# Source Train: (124960, 14)
# Target Train: (124960, 14)
# 이렇게 나와야 하는데..
```

    Source Train: (134408, 14)
    Target Train: (134408, 14)


### 4. 인공지능 만들기


```python
# 모델의 Embedding Size와 Hidden Size를 조절하며
# 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요!
```


```python
vocab_size = tokenizer.num_words + 1  # 단어사전의 단어 개수 + 0:<pad>
embedding_size = 256
hidden_size = 1024

model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(vocab_size, embedding_size))
model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))
model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))
model.add(tf.keras.layers.Dense(vocab_size))

model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding (Embedding)        (None, None, 256)         3072256   
    _________________________________________________________________
    lstm (LSTM)                  (None, None, 1024)        5246976   
    _________________________________________________________________
    lstm_1 (LSTM)                (None, None, 1024)        8392704   
    _________________________________________________________________
    dense (Dense)                (None, None, 12001)       12301025  
    =================================================================
    Total params: 29,012,961
    Trainable params: 29,012,961
    Non-trainable params: 0
    _________________________________________________________________



```python
# 학습 시키자!

optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none'
)

model.compile(loss=loss, optimizer=optimizer)
model.fit(enc_train, dec_train, epochs=10)
```

    Epoch 1/10
    4201/4201 [==============================] - 1036s 246ms/step - loss: 3.5235
    Epoch 2/10
    4201/4201 [==============================] - 1031s 245ms/step - loss: 2.8233
    Epoch 3/10
    4201/4201 [==============================] - 1031s 245ms/step - loss: 2.5321
    Epoch 4/10
    4201/4201 [==============================] - 1031s 245ms/step - loss: 2.2789
    Epoch 5/10
    4201/4201 [==============================] - 1031s 245ms/step - loss: 2.0425
    Epoch 6/10
    1071/4201 [======>.......................] - ETA: 12:48 - loss: 1.8170


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-10-e93c58745d3b> in <module>
          7 
          8 model.compile(loss=loss, optimizer=optimizer)
    ----> 9 model.fit(enc_train, dec_train, epochs=10)
    

    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
       1098                 _r=1):
       1099               callbacks.on_train_batch_begin(step)
    -> 1100               tmp_logs = self.train_function(iterator)
       1101               if data_handler.should_sync:
       1102                 context.async_wait()


    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
        826     tracing_count = self.experimental_get_tracing_count()
        827     with trace.Trace(self._name) as tm:
    --> 828       result = self._call(*args, **kwds)
        829       compiler = "xla" if self._experimental_compile else "nonXla"
        830       new_tracing_count = self.experimental_get_tracing_count()


    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
        853       # In this case we have created variables on the first call, so we run the
        854       # defunned version which is guaranteed to never create variables.
    --> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
        856     elif self._stateful_fn is not None:
        857       # Release the lock early so that multiple threads can perform the call


    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
       2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
       2942     return graph_function._call_flat(
    -> 2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
       2944 
       2945   @property


    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
       1917       # No tape is watching; skip to running the function.
       1918       return self._build_call_outputs(self._inference_function.call(
    -> 1919           ctx, args, cancellation_manager=cancellation_manager))
       1920     forward_backward = self._select_forward_and_backward_functions(
       1921         args,


    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
        558               inputs=args,
        559               attrs=attrs,
    --> 560               ctx=ctx)
        561         else:
        562           outputs = execute.execute_with_cancellation(


    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
         58     ctx.ensure_initialized()
         59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
    ---> 60                                         inputs, attrs, num_outputs)
         61   except core._NotOkStatusException as e:
         62     if name is not None:


    KeyboardInterrupt: 


### 5. 모델 성능 평가


```python
# 작문을 시켜보고 직접 평가하는 방식을 사용하자

# 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 하는 함수

def generate_text(model, tokenizer, init_sentence="<start>", max_len=20):
    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환
    test_input = tokenizer.texts_to_sequences([init_sentence])
    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)
    end_token = tokenizer.word_index["<end>"]

    # 단어 하나씩 예측해 문장을 만든다
    while True:
        
        predict = model(test_tensor)
        # 1 : 입력받은 문장의 텐서를 입력
        
        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] 
        # 2 : 예측된 값 중 가장 높은 확률인 word index를 뽑아낸다
        
        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)
        # 3 : 2에서 예측된 word index를 문장 뒤에 붙인다
        
        if predict_word.numpy()[0] == end_token: break
        if test_tensor.shape[1] >= max_len: break
        # 4 : 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마침
        
    generated = ""
    # tokenizer를 이용해 word index를 단어로 하나씩 변환
    for word_index in test_tensor[0].numpy():
        generated += tokenizer.index_word[word_index] + " "

    return generated
```


```python
# 실행시켜 보자
generate_text(model, tokenizer, init_sentence="<start> he")
```




    '<start> he s a shooting star <end> '




```python
test_loss, test_accuracy = model.evaluate(enc_val,dec_val, verbose=2)
print("test_loss: {} ".format(test_loss))
print("test_accuracy: {}".format(test_accuracy))
```

### 정리

- epoch 결과 -> 가이드가 제시하는 2.2보다 낮은 loss가 나왔다.
- 그런데 이 loss는 validation loss가 아니라 train loss아닌가?
- enc_val과 dec_val을 넣어 evaluate하는 코드를 추가해서 loss를 구해야 가이드가 제시하는 val_loss를 구할 수 있을 것 같은데
- test_loss, test_accuracy = model.evaluate(enc_val,dec_val, verbose=2)코드 실행이 안된다. 우수 작품을 찾아봐야겠다.

### 문제점

- 데이터 전처리가 제대로 되지 않았나보다. Source와 Target Train 데이터가 (124960, 14)로 나와야 한다는데 이 숫자보다 계속 높게 나온다. 우수 작품을 참고해서 부족한 점을 보완해야겠다.

- 이번 노드는 생소한 단어, 개념들이 많아 이해하기 무척 힘들었다. 결과적으로.. 그냥 코드 복사 붙여넣기 + 추가할 코드만 뚜닥뚜닥 수행한 것 같다. NLP 개념은 흥미로운데 처음이라 그런지 조금 낯설다.
